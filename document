1.考虑一个二分问题，即将实例分成正类（positive）或负类（negative）。对一个二分问题来说，会出现四种情况。如果一个实例是正类并且也被 预测成正类，即为真正类（True positive）,如果实例是负类被预测成正类，称之为假正类（False positive）。相应地，如果实例是负类被预测成负类，称之为真负类（True negative）,正类被预测成负类则为假负类（false negative）。 

TP：正确肯定的数目； 

FN：漏报，没有正确找到的匹配的数目； 

FP：误报，给出的匹配是不正确的； 

TN：正确拒绝的非匹配对数 




精确率，precision = TP / (TP + FP) 

模型判为正的所有样本中有多少是真正的正样本 

召回率，recall = TP / (TP + FN) 

准确率，accuracy = (TP + TN) / (TP + FP + TN + FN) 

反映了分类器统对整个样本的判定能力——能将正的判定为正，负的判定为负 

如何在precision和Recall中权衡？ 

F1 Score = P*R/2(P+R)，其中P和R分别为 precision 和 recall 

在precision与recall都要求高的情况下，可以用F1 Score来衡量

为什么会有这么多指标呢？ 

这是因为模式分类和机器学习的需要。判断一个分类器对所用样本的分类能力或者在不同的应用场合时，需要有不同的指标。 当总共有个100 个样本（P+N=100）时，假如只有一个正例（P=1），那么只考虑精确度的话，不需要进行任何模型的训练，直接将所有测试样本判为正例，那么 A 能达到 99%，非常高了，但这并没有反映出模型真正的能力。另外在统计信号分析中，对不同类的判断结果的错误的惩罚是不一样的。举例而言，雷达收到100个来袭导弹的信号，其中只有 3个是真正的导弹信号，其余 97 个是敌方模拟的导弹信号。假如系统判断 98 个（97 个模拟信号加一个真正的导弹信号）信号都是模拟信号，那么Accuracy=98%，很高了，剩下两个是导弹信号，被截掉，这时Recall=2/3=66.67%，Precision=2/2=100%，Precision也很高。但剩下的那颗导弹就会造成灾害。

ROC曲线和AUC 

有时候我们需要在精确率与召回率间进行权衡， 

调整分类器threshold取值，以FPR（假正率False-positive rate）为横坐标，TPR（True-positive rate）为纵坐标做ROC曲线； 

Area Under roc Curve(AUC)：处于ROC curve下方的那部分面积的大小通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的性能； 

精确率和召回率是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下准精确率、召回率就低，召回率低、精确率高，当然如果两者都低，那是什么地方出问题了 
2 Bisecting k-means聚类算法实现 http://shiyanjun.cn/archives/1388.html?utm_source=tuicool&utm_medium=referral
Bisecting k-means聚类算法，即二分k均值算法，它是k-means聚类算法的一个变体，主要是为了改进k-means算法随机选择初始质心的随机性造成聚类结果不确定性的问题，而Bisecting k-means算法受随机选择初始质心的影响比较小。
首先，我们考虑在欧几里德空间中，衡量簇的质量通常使用如下度量：误差平方和（Sum of the Squared Error，简称SSE），也就是要计算执行聚类分析后，对每个点都要计算一个误差值，即非质心点到最近的质心的距离。那么，既然每个非质心点都已经属于某个簇，也就是要计算每个非质心点到其所在簇的质心的距离，最后将这些距离值相加求和，作为SSE去评估一个聚类的质量如何。我们的最终目标是，使得最终的SSE能够最小，也就是一个最小化目标SSE的问题。
Bisecting k-means聚类算法的基本思想是，通过引入局部二分试验，每次试验都通过二分具有最大SSE值的一个簇，二分这个簇以后得到的2个子簇，选择2个子簇的总SSE最小的划分方法，这样能够保证每次二分得到的2个簇是比较优的（也可能是最优的），也就是这2个簇的划分可能是局部最优的，取决于试验的次数。
Bisecting k-means聚类算法的具体执行过程，描述如下所示：

初始时，将待聚类数据集D作为一个簇C0，即C={C0}，输入参数为：二分试验次数m、k-means聚类的基本参数；
取C中具有最大SSE的簇Cp，进行二分试验m次：调用k-means聚类算法，取k=2，将Cp分为2个簇：Ci1、Ci2，一共得到m个二分结果集合B={B1,B2,…,Bm}，其中，Bi={Ci1,Ci2}，这里Ci1和Ci2为每一次二分试验得到的2个簇；
计算上一步二分结果集合B中，每一个划分方法得到的2个簇的总SSE值，选择具有最小总SSE的二分方法得到的结果：Bj={Cj1,Cj2}，并将簇Cj1、Cj2加入到集合C，并将Cp从C中移除；
重复步骤2和3，直到得到k个簇，即集合C中有k个簇。
同k-means算法一样，Bisecting k-means算法不适用于非球形簇的聚类，而且不同尺寸和密度的类型的簇，也不太适合。

3 本地向量和矩阵 http://itfish.net/article/48032.html 
  CSC 矩阵 http://www.cnblogs.com/rollenholt/p/5960523.html
  使用CSC格式表示稀疏矩阵
例如我们想创建一下如下的3x3的稀疏矩阵：

    1   0   4
    0   3   5
    2   0   6
我们就可以使用上面的这个api：

    import org.apache.spark.ml.linalg.{Matrix,Matrices}
    val sm: Matrix = Matrices.sparse(3,3, Array(0,2,3,6), Array(0,2,1,0,1,2), Array(1.0,2.0,3.0,4.0,5.0,6.0))
    输出如下：
    sm: org.apache.spark.ml.linalg.Matrix = 3 x 3 CSCMatrix
(0,0) 1.0
(2,0) 2.0
(1,1) 3.0
(0,2) 4.0
(1,2) 5.0
(2,2) 6.0
也就是说上面的3x3的矩阵，可以表示为下面3个数组：

    Array(0, 2, 3, 6)
    Array(0, 2, 1, 0, 1, 2)
    Array(1, 2, 3, 4, 5, 6)
说实话我第一次看到这个api的时候有点蒙。下面因为没太看懂上面三个Array中的第一个Array(0, 2, 3, 6)是怎么的出来的。也翻看了比较权威的资料（本文最下方的参考资料），但是感觉说的比较不清楚，因此下面谈谈我是如何理解的。

我的理解
上面的3个Array：(为了便于书写我没有写1.0，而是直接写为1)

    Array(0, 2, 3, 6)
    Array(0, 2, 1, 0, 1, 2)
    Array(1, 2, 3, 4, 5, 6)
其中第三个Array很好理解。它的值就是按照列，依次按照顺序记录的矩阵中的非零值。

第二个Array也比较好理解，他表示的是每一列，非零元素所在的行号，行号从0开始。比如上面的矩阵中，第一列元素1在第0行，元素2在第2行。

至于第1个Array理解起来稍微麻烦一些。我的总结就是：

第一个Array的元素个数就是（矩阵的列数+1），也就是矩阵是3列，那么这个Array的个数就是4.
第一个元素一直是0。第二个元素是第一列的非零元素的数量
后续的值为前一个值 + 下一列非零元素的数量
上面的总结可能看起来比较模糊，根据上面的例子我来分析一下：

首先矩阵的3x3的，所以第一个Array会有4个元素。第一个元素是0。得到Array（0）。
矩阵第一列有2个非零元素，所以得到Array的第二个元素为2.得到Array（0， 2）
矩阵的第二列有1个非零元素，那么第三个元素的数量为当前Array的最后一个元素加1，也就是2 + 1=3. 得到Array（0，2， 3）
矩阵的第三列有3个非零元素，那么Array的最后一个元素的值为 3 + 3 = 6. 得到Array（0， 2， 3， 6）
  
 4. 决策树 回归和分类决策树 http://www.cnblogs.com/bourneli/archive/2012/11/17/2775405.html
 http://www.cnblogs.com/iamfongbao/archive/2012/12/26/2833629.html
 本章介绍了三种集合纯度计算方法：
基尼不纯度Gini Impurity
熵entropy
方差variance，计算连续数值。
现在需要知道，前两者可以计算枚举类型的纯度，最后一个计算连续数据结合的纯度。都是值越大，越不纯，越小越纯。
对于数值型目标而建立的决策树，成为回归树(regression tree).树的每一次拆分,为的是降低节点目标变量数值的方差(variance),而不是增加分类变量的纯度(purity).(当然,如果理解为一种属性的话,也可以说是增加变量的纯度)
5. 决策树 computeCost 用来计算k 分类的簇数
   http://www.ibm.com/developerworks/cn/opensource/os-cn-spark-practice4/
   如何选择 K
前面提到 K 的选择是 K-means 算法的关键，Spark MLlib 在 KMeansModel 类里提供了 computeCost 方法，该方法通过计算所有数据点到其最近的中心点的平方和来评估聚类的效果。一般来说，同样的迭代次数和算法跑的次数，这个值越小代表聚类的效果越好。但是在实际情况下，我们还要考虑到聚类结果的可解释性，不能一味的选择使 computeCost 结果值最小的那个 K。
清单 3. K 选择示例代码片段
val ks:Array[Int] = Array(3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
ks.foreach(cluster => {
 val model:KMeansModel = KMeans.train(parsedTrainingData, cluster,30,1)
 val ssd = model.computeCost(parsedTrainingData)
 println("sum of squared distances of points to their nearest center when k=" + cluster + " -> "+ ssd)
})

6.FPGrowth算法 关联规则
  http://www.itdadao.com/articles/c15a677397p0.html
7 关系图 http://images.cnblogs.com/cnblogs_com/wgp13x/752560/o_mllib.jpg
         https://my.oschina.net/u/2306127/blog/743217
8 GaussianMixture 高斯混合模型 高斯混合给出每个数据点被分配到每个cluster的概率
   GMM和k-means其实是十分相似的，区别仅仅在于对GMM来说，我们引入了概率。说到这里，我想先补充一点东西。
   统计学习的模型有两种，一种是概率模型，一种是非概率模型。所谓概率模型，就是指我们要学习的模型的形式是P(Y|X)，
   这样在分类的过程中，我们通过未知数据X可以获得Y取值的一个概率分布，也就是训练后模型得到的输出不是一个具体的值，
   而是一系列值的概率（对应于分类问题来说，就是对应于各个不同的类的概率），然后我们可以选取概率最大的那个类作为判决对象
   （算软分类soft assignment）。而非概率模型，就是指我们学习的模型是一个决策函数Y=f(X)，
   输入数据X是多少就可以投影得到唯一的一个Y，就是判决结果（算硬分类hard assignment）。
   回到GMM，学习的过程就是训练出几个概率分布，所谓混合高斯模型就是指对样本的概率密度分布进行估计，
   而估计的模型是几个高斯模型加权之和（具体是几个要在模型训练前建立好）。每个高斯模型就代表了一个类（一个Cluster）。
   对样本中的数据分别在几个高斯模型上投影，就会分别得到在各个类上的概率。然后我们可以选取概率最大的类所为判决结果
   http://blog.csdn.net/jwh_bupt/article/details/7663885
9  GradientBoostedTrees 
   随机森林和GBTs都是集成学习算法，它们通过集成多棵决策树来实现强分类器。
   集成学习方法就是基于其他的机器学习算法，并把它们有效的组合起来的一种机器学习算法。组合产生的算法相比其中任何一种算法模型更强大、准确。
   随机森林和梯度提升树(GBTs)。两者之间主要差别在于每棵树训练的顺序。
   随机森林通过对数据随机采样来单独训练每一棵树。这种随机性也使得模型相对于单决策树更健壮，且不易在训练集上产生过拟合。
   GBTs则一次只训练一棵树，后面每一棵新的决策树逐步矫正前面决策树产生的误差。随着树的添加，模型的表达力也愈强。
   http://www.aiuxian.com/article/p-2484084.html
   http://lxw1234.com/archives/2016/01/595.htm
   随机森林与GBTs对比： 
- GBTS一次只训练一颗树，训练时间大于随机森林，随机森林可并行地训练多颗树。 GBTs训练较浅的树，花费的时间也少。 
- 随机森林不易过拟合，训练更多的树减少了过拟合的可能性，GBTs中训练过多的树会增加过拟合的可能性。
（随机森林通过多棵树减少variance方差，GBTs通过多棵树减少bias偏置）。 
- 随机森林易调优，效果会随着数数量的增加单调提升。
  
  问题类型 支持的方法 
二分类 线性SVM, 逻辑回归，决策树，随机森林，GBDT，朴素贝叶斯 
多分类 决策树，随机森林，朴素贝叶斯 
回归 线性最小二乘法，Lasso, 岭回归，决策树，随机森林，GBDT，保序回归
10 HypothesisTesting 假设检验
   MLlib当前支持用于判断拟合度或者独立性的Pearson卡方(chi-squared ( χ2) )检验。
   不同的输入类型决定了是做拟合度检验还是独立性检验。拟合度检验要求输入为Vector, 独立性检验要求输入是Matrix。
    //卡方检验
    val v1 = Vectors.dense(43.0, 9.0)
    val v2 = Vectors.dense(44.0, 4.0)   
val c1 = Statistics.chiSqTest(v1, v2)
执行结果：
c1: org.apache.spark.mllib.stat.test.ChiSqTestResult =
Chi squared test summary:
method: pearson
degrees of freedom = 1
statistic = 5.482517482517483
pValue = 0.01920757707591003
Strong presumption against null hypothesis: observed follows the same distribution as expected..
结果返回：统计量：pearson、自由度：1、值：5.48、概率：0.019。
Hypothesis testing，假设检验。Spark目前支持皮尔森卡方检测（Pearson’s chi-squared tests），包括适配度检定和独立性检定。

皮尔森卡方检测
皮尔森卡方检测是最著名的卡方检测方法之一，一般提到卡方检测时若无特殊说明则代表使用的是皮尔森卡方检测。
皮尔森卡方检测可以用来进行适配度检测和独立性检测。
适配度检测
适配度检测，Goodness of Fit test，验证一组观察值的次数分配是否异于理论上的分配。
其 H0H0 假设（虚无假设，null hypothesis）为一个样本中已发生事件的次数分配会服从某个特定的理论分配。
通常情况下这个特定的理论分配指的是均匀分配，目前Spark默认的是均匀分配。
独立性检测
独立性检测，independence test，验证从两个变量抽出的配对观察值组是否互相独立。
其虚无假设是：两个变量呈统计独立性。http://blog.selfup.cn/1157.html
11 kolmogorovSmirnovTest
   KolmogorovSmirnovTest[data] 
   使用 Kolmogorov-Smirnov 检验判断 data 是否服从正态分布.
   KolmogorovSmirnovTest[data,dist] 
   使用 Kolmogorov-Smirnov 检验判断 data 是否服从 dist 分布.
   KolmogorovSmirnovTest[data,dist,"property"] 
   返回 "property" 的值.
    在统计学中，柯尔莫可洛夫-斯米洛夫检验基于累计分布函数，用以检验两个经验分布是否不同或一个经验分布与另一个理想分布是否不同。

在进行累计概率（cumulative
probability）统计的时候，你怎么知道组之间是否有显著性差异？有人首先想到单因素方差分析或双尾检验（2 tailed test）。
其实这些是不准确的，最好采用Kolmogorov-Smirnov test（柯尔莫诺夫-斯米尔诺夫检验）
来分析变量是否符合某种分布或比较两组之间有无显著性差异。
Kolmogorov-Smirnov检验
它是检验单一样本是否来自某一特定分布的方法。比如检验一组数据是否为正态分布。
它的检验方法是以样本数据的累计频数分布与特定理论分布比较，若两者间的差距很小，则推论该样本取自某特定分布族。
http://www.aichengxu.com/view/6860682
12 保序回归(Isotonic Regression)
   对生成的数据进行保序回归的一个实例.保序回归能在训练数据上发现一个非递减逼近函数的同时最小化均方误差。
   这种回归，是这一种单调函数的回归，回归模型中后一个x一定比前一个x大，也就是有序，
   保序回归的应用之一就是用来做统计推断，比如药量和毒性的关系，一般认为毒性随着药量是不减或者递增的关系，借此可以来估计最大药量。
   
13  KernelDensityEstimation 
    对于数值属性，如果不服从正态分布，但不知道服从何种分布形式，可以采用核密度估计的方法来进行预测。
    val sample = sc.parallelize(Seq(0.0, 1.0, 4.0, 4.0))
  val kernelDensity=new KernelDensity()
                          .setSample(sample) //设置密度估计样本
                          .setBandwidth(3.0) //设置带宽，对高斯核函数来讲就是标准差
  //给定相应的点，估计其概率密度
  //densities: Array[Double] = 
  //Array(0.07464879256673691, 0.1113106036883375, 0.08485447240456075)
  val densities = kernelDensity.estimate(Array(-1.0, 2.0, 5.0))
   原理比较简单，在我们知道某一事物的概率分布的情况下
   如果某一个数在观察中出现了，我们认为这个数的概率密度很大，和这个数近的数的概率密度也比较大；而那些离这个数远的数的概率密度会比较小。
    基于这种想法，针对观察中的第一个数，我们可以用 K 去拟合我们想象中的那个远小近大概率密度。
    对每一个观察数拟合出的多个概率密度分布函数，取平均。 如果某些数是比较重要的，则可以取加权平均。
   核密度的估计并不是找到真正的分布函数。
   http://www.cnblogs.com/skyEva/p/5563662.html
14  LDA http://blog.csdn.net/pirage/article/details/50219323
    http://www.infocool.net/kb/Spark/201610/205164.html
15  LogisticRegressionWithLBFGS  http://blog.csdn.net/sinat_33761963/article/details/50779577
16  MovieLensALS 最小二乘法http://itindex.net/detail/54761-spark-mllib-%E8%B1%86%E7%93%A3
