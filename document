1.考虑一个二分问题，即将实例分成正类（positive）或负类（negative）。对一个二分问题来说，会出现四种情况。如果一个实例是正类并且也被 预测成正类，即为真正类（True positive）,如果实例是负类被预测成正类，称之为假正类（False positive）。相应地，如果实例是负类被预测成负类，称之为真负类（True negative）,正类被预测成负类则为假负类（false negative）。 

TP：正确肯定的数目； 

FN：漏报，没有正确找到的匹配的数目； 

FP：误报，给出的匹配是不正确的； 

TN：正确拒绝的非匹配对数 




精确率，precision = TP / (TP + FP) 

模型判为正的所有样本中有多少是真正的正样本 

召回率，recall = TP / (TP + FN) 

准确率，accuracy = (TP + TN) / (TP + FP + TN + FN) 

反映了分类器统对整个样本的判定能力——能将正的判定为正，负的判定为负 

如何在precision和Recall中权衡？ 

F1 Score = P*R/2(P+R)，其中P和R分别为 precision 和 recall 

在precision与recall都要求高的情况下，可以用F1 Score来衡量

为什么会有这么多指标呢？ 

这是因为模式分类和机器学习的需要。判断一个分类器对所用样本的分类能力或者在不同的应用场合时，需要有不同的指标。 当总共有个100 个样本（P+N=100）时，假如只有一个正例（P=1），那么只考虑精确度的话，不需要进行任何模型的训练，直接将所有测试样本判为正例，那么 A 能达到 99%，非常高了，但这并没有反映出模型真正的能力。另外在统计信号分析中，对不同类的判断结果的错误的惩罚是不一样的。举例而言，雷达收到100个来袭导弹的信号，其中只有 3个是真正的导弹信号，其余 97 个是敌方模拟的导弹信号。假如系统判断 98 个（97 个模拟信号加一个真正的导弹信号）信号都是模拟信号，那么Accuracy=98%，很高了，剩下两个是导弹信号，被截掉，这时Recall=2/3=66.67%，Precision=2/2=100%，Precision也很高。但剩下的那颗导弹就会造成灾害。

ROC曲线和AUC 

有时候我们需要在精确率与召回率间进行权衡， 

调整分类器threshold取值，以FPR（假正率False-positive rate）为横坐标，TPR（True-positive rate）为纵坐标做ROC曲线； 

Area Under roc Curve(AUC)：处于ROC curve下方的那部分面积的大小通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的性能； 

精确率和召回率是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下准精确率、召回率就低，召回率低、精确率高，当然如果两者都低，那是什么地方出问题了 
2 Bisecting k-means聚类算法实现 http://shiyanjun.cn/archives/1388.html?utm_source=tuicool&utm_medium=referral
Bisecting k-means聚类算法，即二分k均值算法，它是k-means聚类算法的一个变体，主要是为了改进k-means算法随机选择初始质心的随机性造成聚类结果不确定性的问题，而Bisecting k-means算法受随机选择初始质心的影响比较小。
首先，我们考虑在欧几里德空间中，衡量簇的质量通常使用如下度量：误差平方和（Sum of the Squared Error，简称SSE），也就是要计算执行聚类分析后，对每个点都要计算一个误差值，即非质心点到最近的质心的距离。那么，既然每个非质心点都已经属于某个簇，也就是要计算每个非质心点到其所在簇的质心的距离，最后将这些距离值相加求和，作为SSE去评估一个聚类的质量如何。我们的最终目标是，使得最终的SSE能够最小，也就是一个最小化目标SSE的问题。
Bisecting k-means聚类算法的基本思想是，通过引入局部二分试验，每次试验都通过二分具有最大SSE值的一个簇，二分这个簇以后得到的2个子簇，选择2个子簇的总SSE最小的划分方法，这样能够保证每次二分得到的2个簇是比较优的（也可能是最优的），也就是这2个簇的划分可能是局部最优的，取决于试验的次数。
Bisecting k-means聚类算法的具体执行过程，描述如下所示：

初始时，将待聚类数据集D作为一个簇C0，即C={C0}，输入参数为：二分试验次数m、k-means聚类的基本参数；
取C中具有最大SSE的簇Cp，进行二分试验m次：调用k-means聚类算法，取k=2，将Cp分为2个簇：Ci1、Ci2，一共得到m个二分结果集合B={B1,B2,…,Bm}，其中，Bi={Ci1,Ci2}，这里Ci1和Ci2为每一次二分试验得到的2个簇；
计算上一步二分结果集合B中，每一个划分方法得到的2个簇的总SSE值，选择具有最小总SSE的二分方法得到的结果：Bj={Cj1,Cj2}，并将簇Cj1、Cj2加入到集合C，并将Cp从C中移除；
重复步骤2和3，直到得到k个簇，即集合C中有k个簇。
同k-means算法一样，Bisecting k-means算法不适用于非球形簇的聚类，而且不同尺寸和密度的类型的簇，也不太适合。

3 本地向量和矩阵 http://itfish.net/article/48032.html 
  CSC 矩阵 http://www.cnblogs.com/rollenholt/p/5960523.html
  使用CSC格式表示稀疏矩阵
例如我们想创建一下如下的3x3的稀疏矩阵：

    1   0   4
    0   3   5
    2   0   6
我们就可以使用上面的这个api：

    import org.apache.spark.ml.linalg.{Matrix,Matrices}
    val sm: Matrix = Matrices.sparse(3,3, Array(0,2,3,6), Array(0,2,1,0,1,2), Array(1.0,2.0,3.0,4.0,5.0,6.0))
    输出如下：
    sm: org.apache.spark.ml.linalg.Matrix = 3 x 3 CSCMatrix
(0,0) 1.0
(2,0) 2.0
(1,1) 3.0
(0,2) 4.0
(1,2) 5.0
(2,2) 6.0
也就是说上面的3x3的矩阵，可以表示为下面3个数组：

    Array(0, 2, 3, 6)
    Array(0, 2, 1, 0, 1, 2)
    Array(1, 2, 3, 4, 5, 6)
说实话我第一次看到这个api的时候有点蒙。下面因为没太看懂上面三个Array中的第一个Array(0, 2, 3, 6)是怎么的出来的。也翻看了比较权威的资料（本文最下方的参考资料），但是感觉说的比较不清楚，因此下面谈谈我是如何理解的。

我的理解
上面的3个Array：(为了便于书写我没有写1.0，而是直接写为1)

    Array(0, 2, 3, 6)
    Array(0, 2, 1, 0, 1, 2)
    Array(1, 2, 3, 4, 5, 6)
其中第三个Array很好理解。它的值就是按照列，依次按照顺序记录的矩阵中的非零值。

第二个Array也比较好理解，他表示的是每一列，非零元素所在的行号，行号从0开始。比如上面的矩阵中，第一列元素1在第0行，元素2在第2行。

至于第1个Array理解起来稍微麻烦一些。我的总结就是：

第一个Array的元素个数就是（矩阵的列数+1），也就是矩阵是3列，那么这个Array的个数就是4.
第一个元素一直是0。第二个元素是第一列的非零元素的数量
后续的值为前一个值 + 下一列非零元素的数量
上面的总结可能看起来比较模糊，根据上面的例子我来分析一下：

首先矩阵的3x3的，所以第一个Array会有4个元素。第一个元素是0。得到Array（0）。
矩阵第一列有2个非零元素，所以得到Array的第二个元素为2.得到Array（0， 2）
矩阵的第二列有1个非零元素，那么第三个元素的数量为当前Array的最后一个元素加1，也就是2 + 1=3. 得到Array（0，2， 3）
矩阵的第三列有3个非零元素，那么Array的最后一个元素的值为 3 + 3 = 6. 得到Array（0， 2， 3， 6）
  
 4. 决策树 回归和分类决策树 http://www.cnblogs.com/bourneli/archive/2012/11/17/2775405.html
 http://www.cnblogs.com/iamfongbao/archive/2012/12/26/2833629.html
 本章介绍了三种集合纯度计算方法：
基尼不纯度Gini Impurity
熵entropy
方差variance，计算连续数值。
现在需要知道，前两者可以计算枚举类型的纯度，最后一个计算连续数据结合的纯度。都是值越大，越不纯，越小越纯。
对于数值型目标而建立的决策树，成为回归树(regression tree).树的每一次拆分,为的是降低节点目标变量数值的方差(variance),而不是增加分类变量的纯度(purity).(当然,如果理解为一种属性的话,也可以说是增加变量的纯度)
