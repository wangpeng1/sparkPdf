1.考虑一个二分问题，即将实例分成正类（positive）或负类（negative）。对一个二分问题来说，会出现四种情况。如果一个实例是正类并且也被 预测成正类，即为真正类（True positive）,如果实例是负类被预测成正类，称之为假正类（False positive）。相应地，如果实例是负类被预测成负类，称之为真负类（True negative）,正类被预测成负类则为假负类（false negative）。 

TP：正确肯定的数目； 

FN：漏报，没有正确找到的匹配的数目； 

FP：误报，给出的匹配是不正确的； 

TN：正确拒绝的非匹配对数 




精确率，precision = TP / (TP + FP) 

模型判为正的所有样本中有多少是真正的正样本 

召回率，recall = TP / (TP + FN) 

准确率，accuracy = (TP + TN) / (TP + FP + TN + FN) 

反映了分类器统对整个样本的判定能力——能将正的判定为正，负的判定为负 

如何在precision和Recall中权衡？ 

F1 Score = P*R/2(P+R)，其中P和R分别为 precision 和 recall 

在precision与recall都要求高的情况下，可以用F1 Score来衡量

为什么会有这么多指标呢？ 

这是因为模式分类和机器学习的需要。判断一个分类器对所用样本的分类能力或者在不同的应用场合时，需要有不同的指标。 当总共有个100 个样本（P+N=100）时，假如只有一个正例（P=1），那么只考虑精确度的话，不需要进行任何模型的训练，直接将所有测试样本判为正例，那么 A 能达到 99%，非常高了，但这并没有反映出模型真正的能力。另外在统计信号分析中，对不同类的判断结果的错误的惩罚是不一样的。举例而言，雷达收到100个来袭导弹的信号，其中只有 3个是真正的导弹信号，其余 97 个是敌方模拟的导弹信号。假如系统判断 98 个（97 个模拟信号加一个真正的导弹信号）信号都是模拟信号，那么Accuracy=98%，很高了，剩下两个是导弹信号，被截掉，这时Recall=2/3=66.67%，Precision=2/2=100%，Precision也很高。但剩下的那颗导弹就会造成灾害。

ROC曲线和AUC 

有时候我们需要在精确率与召回率间进行权衡， 

调整分类器threshold取值，以FPR（假正率False-positive rate）为横坐标，TPR（True-positive rate）为纵坐标做ROC曲线； 

Area Under roc Curve(AUC)：处于ROC curve下方的那部分面积的大小通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的性能； 

精确率和召回率是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下准精确率、召回率就低，召回率低、精确率高，当然如果两者都低，那是什么地方出问题了 
